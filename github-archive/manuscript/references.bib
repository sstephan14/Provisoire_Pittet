@Book{R-knitr,
  title = {Dynamic Documents with {R} and knitr},
  author = {Yihui Xie},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2015},
  edition = {2nd},
  note = {ISBN 978-1498716963},
  url = {https://yihui.org/knitr/},
}
@Manual{R-papaja,
  author = {Frederik Aust and Marius Barth},
  title = {{papaja}: {Create} {APA} manuscripts with {R Markdown}},
  year = {2020},
  note = {R package version 0.1.0.9942},
  url = {https://github.com/crsh/papaja},
}
@TechReport{bollen2015,
  title = {Social, {{Behavioral}}, and {{Economic Sciences Perspectives}} on {{Robust}} and {{Reliable Science}}},
  author = {K Bollen and J. T. Cacioppo and R. M. Kaplan and J. A. Krosnick and J. L. Olds},
  year = {2015},
  address = {{Arlington, VA}},
  institution = {{National Science Foundation}},
  file = {/Users/tomhardwicke/Zotero/storage/QQTY79WT/SBE_Robust_and_Reliable_Research_Report.pdf},
}
@Article{stodden2018,
  title = {An Empirical Analysis of Journal Policy Effectiveness for Computational Reproducibility},
  author = {Victoria Stodden and Jennifer Seiler and Zhaokun Ma},
  year = {2018},
  month = {mar},
  volume = {115},
  pages = {2584--2589},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708290115},
  abstract = {A key component of scientific communication is sufficient information for other researchers in the field to reproduce published findings. For computational and data-enabled research, this has often been interpreted to mean making available the raw data from which results were generated, the computer code that generated the findings, and any additional information needed such as workflows and input parameters. Many journals are revising author guidelines to include data and code availability. This work evaluates the effectiveness of journal policy that requires the data and code necessary for reproducibility be made available postpublication by the authors upon request. We assess the effectiveness of such a policy by (
              i
              ) requesting data and code from authors and (
              ii
              ) attempting replication of the published findings. We chose a random sample of 204 scientific papers published in the journal
              Science
              after the implementation of their policy in February 2011. We found that we were able to obtain artifacts from 44\% of our sample and were able to reproduce the findings for 26\%. We find this policy\textemdash author remission of data and code postpublication upon request\textemdash an improvement over no policy, but currently insufficient for reproducibility.},
  file = {/Users/tomhardwicke/Zotero/storage/RURGBULE/Stodden et al. - 2018 - An empirical analysis of journal policy effectiven.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {11},
}
@Article{hardwicke2018a,
  title = {Data Availability, Reusability, and Analytic Reproducibility: Evaluating the Impact of a Mandatory Open Data Policy at the Journal {{Cognition}}},
  shorttitle = {Data Availability, Reusability, and Analytic Reproducibility},
  author = {Tom E. Hardwicke and Maya B. Mathur and Kyle MacDonald and Gustav Nilsonne and George C. Banks and Mallory C. Kidwell and Alicia {Hofelich Mohr} and Elizabeth Clayton and Erica J. Yoon and Michael {Henry Tessler} and Richie L. Lenne and Sara Altman and Bria Long and Michael C. Frank},
  year = {2018},
  volume = {5},
  pages = {180448},
  publisher = {{Royal Society}},
  doi = {10.1098/rsos.180448},
  abstract = {Access to data is a critical feature of an efficient, progressive and ultimately self-correcting scientific ecosystem. But the extent to which in-principle benefits of data sharing are realized in practice is unclear. Crucially, it is largely unknown whether published findings can be reproduced by repeating reported analyses upon shared data (`analytic reproducibility'). To investigate this, we conducted an observational evaluation of a mandatory open data policy introduced at the journal Cognition. Interrupted time-series analyses indicated a substantial post-policy increase in data available statements (104/417, 25\% pre-policy to 136/174, 78\% post-policy), although not all data appeared reusable (23/104, 22\% pre-policy to 85/136, 62\%, post-policy). For 35 of the articles determined to have reusable data, we attempted to reproduce 1324 target values. Ultimately, 64 values could not be reproduced within a 10\% margin of error. For 22 articles all target values were reproduced, but 11 of these required author assistance. For 13 articles at least one value could not be reproduced despite author assistance. Importantly, there were no clear indications that original conclusions were seriously impacted. Mandatory open data policies can increase the frequency and quality of data sharing. However, suboptimal data curation, unclear analysis specification and reporting errors can impede analytic reproducibility, undermining the utility of data sharing and the credibility of scientific findings.},
  file = {/Users/tomhardwicke/Zotero/storage/JK4NV4RJ/Hardwicke et al. - Data availability, reusability, and analytic repro.pdf;/Users/tomhardwicke/Zotero/storage/WSB7QBPS/rsos.html},
  journal = {Royal Society Open Science},
  number = {8},
}
@Article{nuijten2018,
  title = {Verify Original Results through Reanalysis before Replicating},
  author = {Mich{\a`e}le B. Nuijten and Marjan Bakker and Esther Maassen and Jelte M. Wicherts},
  year = {2018},
  volume = {41},
  publisher = {{Cambridge University Press}},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X18000791},
  abstract = {In determining the need to directly replicate, it is crucial to first verify the original results through independent reanalysis of the data. Original results that appear erroneous and that cannot be reproduced by reanalysis offer little evidence to begin with, thereby diminishing the need to replicate. Sharing data and scripts is essential to ensure reproducibility.},
  file = {/Users/tomhardwicke/Zotero/storage/TT282U3B/0CB8760C66146E0A567E974573BBC2D8.html},
  journal = {Behavioral and Brain Sciences},
  language = {en},
}
@Article{lebel2018,
  title = {A {{Unified Framework}} to {{Quantify}} the {{Credibility}} of {{Scientific Findings}}:},
  shorttitle = {A {{Unified Framework}} to {{Quantify}} the {{Credibility}} of {{Scientific Findings}}},
  author = {Etienne P. LeBel and Randy J. McCarthy and Brian D. Earp and Malte Elson and Wolf Vanpaemel},
  year = {2018},
  month = {aug},
  publisher = {{SAGE PublicationsSage CA: Los Angeles, CA}},
  doi = {10.1177/2515245918787489},
  abstract = {Societies invest in scientific studies to better understand the world and attempt to harness such improved understanding to address pressing societal problems. ...},
  file = {/Users/tomhardwicke/Zotero/storage/2KM4CC52/LeBel et al. - 2018 - A Unified Framework to Quantify the Credibility of.pdf;/Users/tomhardwicke/Zotero/storage/E3DBWBDK/2515245918787489.html},
  journal = {Advances in Methods and Practices in Psychological Science},
  language = {en},
}
@TechReport{chang2015,
  title = {Is {{Economics Research Replicable}}? {{Sixty Published Papers}} from {{Thirteen Journals Say}} {"}{{Usually Not}}{"}},
  shorttitle = {Is {{Economics Research Replicable}}?},
  author = {A. C. Chang and Phillip Li},
  year = {2015},
  month = {oct},
  pages = {1--26},
  address = {{Washington}},
  institution = {{Board of Governors of the Federal Reserve System}},
  abstract = {We attempt to replicate 67 papers published in 13 well-regarded economics journals using author-provided replication files that include both data and code. Some journals in our sample require data and code replication files, and other journals do not require such files. Aside from 6 papers that use confidential data, we obtain data and code replication files for 29 of 35 papers (83\%) that are required to provide such files as a condition of publication, compared to 11 of 26 papers (42\%) that are not required to provide data and code replication files. We successfully replicate the key qualitative result of 22 of 67 papers (33\%) without contacting the authors. Excluding the 6 papers that use confidential data and the 2 papers that use software we do not possess, we replicate 29 of 59 papers (49\%) with assistance from the authors. Because we are able to replicate less than half of the papers in our sample even with help from the authors, we assert that economics research is usually not replicable. We conclude with recommendations on improving replication of economics research.},
  file = {/Users/tomhardwicke/Zotero/storage/WZ66RK48/Board of Governors of the Federal Reserve System et al. - 2015 - Is Economics Research Replicable Sixty Published .pdf},
  language = {en},
}
@Article{eubank2016,
  title = {Lessons from a {{Decade}} of {{Replications}} at the {{Quarterly Journal}} of {{Political Science}}},
  author = {Nicholas Eubank},
  year = {2016},
  month = {apr},
  volume = {49},
  pages = {273--276},
  publisher = {{Cambridge University Press}},
  issn = {1049-0965, 1537-5935},
  doi = {10.1017/S1049096516000196},
  abstract = {To allow researchers to investigate not only whether a paper's methods are theoretically sound but also whether they have been properly implemented and are robust to alternative specifications, it is necessary that published papers be accompanied by their underlying data and code. This article describes experiences and lessons learned at the Quarterly Journal of Political Science since it began requiring authors to provide this type of replication code in 2005. It finds that of the 24 empirical papers subjected to in-house replication review since September 2012, only four packages did not require any modifications. Most troubling, 14 packages (58\%) had results in the paper that differed from those generated by the author's own code. Based on these experiences, this article presents a set of guidelines for authors and journals for improving the reliability and usability of replication packages.},
  file = {/Users/tomhardwicke/Zotero/storage/NK94JDLL/Eubank - 2016 - Lessons from a Decade of Replications at the Quart.pdf;/Users/tomhardwicke/Zotero/storage/AG2VYKVD/284B2830BFD99888B42D4CEABC28B9EE.html},
  journal = {PS: Political Science \& Politics},
  language = {en},
  number = {2},
}
@Article{obels2019,
  title = {Analysis of {{Open Data}} and {{Computational Reproducibility}} in {{Registered Reports}} in {{Psychology}}},
  author = {Pepijn Obels and Daniel Lakens and Nicholas Coles and Jaroslav Gottfried and Seth Ariel Green},
  year = {2019},
  month = {may},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/fk8vh},
  abstract = {Ongoing technological developments have made it easier than ever before for scientists to share their data, materials, and analysis code. Sharing data and analysis code makes it easier for other researchers to re-use or check published research. However, these benefits will only emerge if researchers can reproduce the analysis reported in published articles and if data is annotated well enough so that it is clear what all variables mean. Because most researchers are not trained in computational reproducibility, it is important to evaluate current practices to identify practices that can be improved. We examined data and code sharing for Registered Reports published in the psychological literature between 2014 and 2018, and attempted to independently computationally reproduce the main results in each article. Of the main results from 62 articles that met our inclusion criteria, data were available for 41 articles, and analysis scripts for 37 articles. For the main results in 36 articles that shared both data and code we could run the scripts for 31 analyses, and reproduce the main results for 21 articles. Although the articles that shared both data and code (36 out of 62, or 58\%) and articles for which main results could be computationally reproduced (21 out of 36, or 58\%) was relatively high compared to other studies, there is clear room for improvement. We provide practical recommendations based on our observations and link to examples of good research practices in the papers we reproduced.},
  file = {/Users/tomhardwicke/pCloud Drive/Zotero_Library/Obels_etal_2019.pdf;/Users/tomhardwicke/Zotero/storage/Y633F6DF/fk8vh.html},
}
@Article{hardwicke2018,
  title = {Populating the {{Data Ark}}: {{An}} Attempt to Retrieve, Preserve, and Liberate Data from the Most Highly-Cited Psychology and Psychiatry Articles},
  shorttitle = {Populating the {{Data Ark}}},
  author = {Tom E. Hardwicke and John P. A. Ioannidis},
  year = {2018},
  month = {aug},
  volume = {13},
  pages = {e0201856},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0201856},
  abstract = {The vast majority of scientific articles published to-date have not been accompanied by concomitant publication of the underlying research data upon which they are based. This state of affairs precludes the routine re-use and re-analysis of research data, undermining the efficiency of the scientific enterprise, and compromising the credibility of claims that cannot be independently verified. It may be especially important to make data available for the most influential studies that have provided a foundation for subsequent research and theory development. Therefore, we launched an initiative\textemdash the Data Ark\textemdash to examine whether we could retrospectively enhance the preservation and accessibility of important scientific data. Here we report the outcome of our efforts to retrieve, preserve, and liberate data from 111 of the most highly-cited articles published in psychology and psychiatry between 2006\textendash 2011 (n = 48) and 2014\textendash 2016 (n = 63). Most data sets were not made available (76/111, 68\%, 95\% CI [60, 77]), some were only made available with restrictions (20/111, 18\%, 95\% CI [10, 27]), and few were made available in a completely unrestricted form (15/111, 14\%, 95\% CI [5, 22]). Where extant data sharing systems were in place, they usually (17/22, 77\%, 95\% CI [54, 91]) did not allow unrestricted access. Authors reported several barriers to data sharing, including issues related to data ownership and ethical concerns. The Data Ark initiative could help preserve and liberate important scientific data, surface barriers to data sharing, and advance community discussions on data stewardship.},
  file = {/Users/tomhardwicke/Zotero/storage/5ECEZNXB/Hardwicke and Ioannidis - 2018 - Populating the Data Ark An attempt to retrieve, p.pdf;/Users/tomhardwicke/Zotero/storage/ZVL996IY/article.html},
  journal = {PLOS ONE},
  keywords = {Attention,Citation analysis,Data acquisition,Health care policy,Mental health and psychiatry,Open science,Psychology,Science policy},
  language = {en},
  number = {8},
}
@Article{hardwicke2020a,
  title = {Estimating the Prevalence of Transparency and Reproducibility-Related Research Practices in Psychology (2014-2017)},
  author = {Tom E. Hardwicke and Robert T. Thibault and Jessica Kosie and Joshua D. Wallach and Mallory C. Kidwell and John Ioannidis},
  year = {2020},
  month = {jan},
  publisher = {{MetaArXiv}},
  doi = {10.31222/osf.io/9sz2y},
  abstract = {Psychological science is navigating an unprecedented period of introspection about the credibility and utility of its research. A number of reform initiatives aimed at increasing adoption of transparency and reproducibility-related research practices appear to have been effective in specific contexts; however, their broader, collective impact amidst a wider discussion about research credibility and reproducibility is largely unknown. In the present study, we estimated the prevalence of several transparency and reproducibility-related indicators in the psychology literature published between 2014-2017 by manually assessing these indicators in a random sample of 250 articles. Over half of the articles we examined were publicly available (154/237, 65\% [95\% confidence interval, 59\% to 71\%]). However, sharing of important research resources such as materials (26/183, 14\% [10\% to 19\%]), study protocols (0/188, 0\% [0\% to 1\%]), raw data (4/188, 2\% [1\% to 4\%]), and analysis scripts (1/188, 1\% [0\% to 1\%]) was rare. Pre-registration was also uncommon (5/188, 3\% [1\% to 5\%]). Although many articles included a funding disclosure statement (142/228, 62\% [56\% to 69\%]), conflict of interest disclosure statements were less common (88/228, 39\% [32\% to 45\%]). Replication studies were rare (10/188, 5\% [3\% to 8\%]) and few studies were included in systematic reviews (21/183, 11\% [8\% to 16\%]) or meta-analyses (12/183, 7\% [4\% to 10\%]). Overall, the findings suggest that transparent and reproducibility-related research practices are far from routine in psychological science. Future studies can use the present findings as a baseline to assess progress towards increasing the credibility and utility of psychology research.},
  file = {/Users/tomhardwicke/Zotero/storage/7P6B8QYX/Hardwicke et al. - 2020 - Estimating the prevalence of transparency and repr.pdf;/Users/tomhardwicke/Zotero/storage/NWJHYJFM/9sz2y.html},
}
@Article{wicherts2006,
  title = {The Poor Availability of Psychological Research Data for Reanalysis.},
  author = {Jelte M. Wicherts and Denny Borsboom and Judith Kats and Dylan Molenaar},
  year = {2006},
  volume = {61},
  pages = {726--728},
  issn = {1935-990X, 0003-066X},
  doi = {10.1037/0003-066X.61.7.726},
  file = {/Users/tomhardwicke/Zotero/storage/PT8FVF5W/Wicherts et al. - 2006 - The poor availability of psychological research da.pdf},
  journal = {American Psychologist},
  language = {en},
  number = {7},
}
@Article{kidwell2016,
  title = {Badges to {{Acknowledge Open Practices}}: {{A Simple}}, {{Low}}-{{Cost}}, {{Effective Method}} for {{Increasing Transparency}}},
  shorttitle = {Badges to {{Acknowledge Open Practices}}},
  author = {Mallory C. Kidwell and Ljiljana B. Lazarevi{\a'c} and Erica Baranski and Tom E. Hardwicke and Sarah Piechowski and Lina-Sophia Falkenberg and Curtis Kennett and Agnieszka Slowik and Carina Sonnleitner and Chelsey Hess-Holden and Timothy M. Errington and Susann Fiedler and Brian A. Nosek},
  year = {2016},
  month = {may},
  volume = {14},
  pages = {e1002456},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002456},
  abstract = {Beginning January 2014, Psychological Science gave authors the opportunity to signal open data and materials if they qualified for badges that accompanied published articles. Before badges, less than 3\% of Psychological Science articles reported open data. After badges, 23\% reported open data, with an accelerating trend; 39\% reported open data in the first half of 2015, an increase of more than an order of magnitude from baseline. There was no change over time in the low rates of data sharing among comparison journals. Moreover, reporting openness does not guarantee openness. When badges were earned, reportedly available data were more likely to be actually available, correct, usable, and complete than when badges were not earned. Open materials also increased to a weaker degree, and there was more variability among comparison journals. Badges are simple, effective signals to promote open practices and improve preservation of data and materials by using independent repositories.},
  file = {/Users/tomhardwicke/Zotero/storage/TQ88KP85/Kidwell et al. - 2016 - Badges to Acknowledge Open Practices A Simple, Lo.pdf;/Users/tomhardwicke/Zotero/storage/S28XCETC/article.html},
  journal = {PLOS Biology},
  keywords = {Behavior,Cognitive psychology,Experimental psychology,Open data,Open science,Psychology,Research assessment,Scientific publishing},
  language = {en},
  number = {5},
}
@Article{vines2014,
  title = {The {{Availability}} of {{Research Data Declines Rapidly}} with {{Article Age}}},
  author = {Timothy H. Vines and Arianne Y. K. Albert and Rose L. Andrew and Florence D{\a'e}barre and Dan G. Bock and Michelle T. Franklin and Kimberly J. Gilbert and Jean-S{\a'e}bastien Moore and S{\a'e}bastien Renaut and Diana J. Rennison},
  year = {2014},
  month = {jan},
  volume = {24},
  pages = {94--97},
  publisher = {{Elsevier}},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2013.11.014},
  file = {/Users/tomhardwicke/pCloud Drive/Zotero_Library/Vines_etal_2014.pdf;/Users/tomhardwicke/Zotero/storage/8MVAMVYV/S0960-9822(13)01400-0.html},
  journal = {Current Biology},
  language = {English},
  number = {1},
  pmid = {24361065},
}
@Article{eich2014,
  title = {Business {{Not}} as {{Usual}}},
  author = {Eric Eich},
  year = {2014},
  month = {jan},
  volume = {25},
  pages = {3--6},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797613512465},
  file = {/Users/tomhardwicke/pCloud Drive/Zotero_Library/Eich_2014.pdf},
  journal = {Psychological Science},
  language = {en},
  number = {1},
}
@Article{wicherts2011,
  title = {Willingness to {{Share Research Data Is Related}} to the {{Strength}} of the {{Evidence}} and the {{Quality}} of {{Reporting}} of {{Statistical Results}}},
  author = {Jelte M. Wicherts and Marjan Bakker and Dylan Molenaar},
  editor = {Rochelle E. Tractenberg},
  year = {2011},
  month = {nov},
  volume = {6},
  pages = {e26828},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0026828},
  abstract = {Background: The widespread reluctance to share published research data is often hypothesized to be due to the authors' fear that reanalysis may expose errors in their work or may produce conclusions that contradict their own. However, these hypotheses have not previously been studied systematically.
Methods and Findings: We related the reluctance to share research data for reanalysis to 1148 statistically significant results reported in 49 papers published in two major psychology journals. We found the reluctance to share data to be associated with weaker evidence (against the null hypothesis of no effect) and a higher prevalence of apparent errors in the reporting of statistical results. The unwillingness to share data was particularly clear when reporting errors had a bearing on statistical significance.
Conclusions: Our findings on the basis of psychological papers suggest that statistical results are particularly hard to verify when reanalysis is more likely to lead to contrasting conclusions. This highlights the importance of establishing mandatory data archiving policies.},
  file = {/Users/tomhardwicke/Zotero/storage/GURMMKIB/Wicherts et al. - 2011 - Willingness to Share Research Data Is Related to t.PDF},
  journal = {PLoS ONE},
  language = {en},
  number = {11},
}
@Article{nuijten2017,
  title = {Journal {{Data Sharing Policies}} and {{Statistical Reporting Inconsistencies}} in {{Psychology}}},
  author = {Mich{\a`e}le B. Nuijten and Jeroen Borghuis and Coosje L. S. Veldkamp and Linda Dominguez-Alvarez and Marcel A. L. M. {Van Assen} and Jelte M. Wicherts},
  year = {2017},
  month = {dec},
  volume = {3},
  pages = {31},
  issn = {2474-7394},
  doi = {10.1525/collabra.102},
  file = {/Users/tomhardwicke/Zotero/storage/B43TWINC/Nuijten et al. - 2017 - Journal Data Sharing Policies and Statistical Repo.pdf},
  journal = {Collabra: Psychology},
  language = {en},
  number = {1},
}
@Article{sakaluk2014,
  title = {Analytic {{Review}} as a {{Solution}} to the {{Misreporting}} of {{Statistical Results}} in {{Psychological Science}}},
  author = {John Sakaluk and Alexander Williams and Monica Biernat},
  year = {2014},
  month = {nov},
  volume = {9},
  pages = {652--660},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691614549257},
  abstract = {In this article, we propose analytic review (AR) as a solution to the problem of misreporting statistical results in psychological science. AR requires authors submitting manuscripts for publication to also submit the data file and syntax used during analyses. Regular reviewers or statistical experts then review reported analyses in order to verify that the analyses reported were actually conducted and that the statistical values are accurately reported. We begin by describing the problem of misreporting in psychology and introduce the basic AR process. We then highlight both primary and secondary benefits of adopting AR and describe different permutations of the AR system, each of which has its own strengths and limitations. We conclude by attempting to dispel three anticipated concerns about AR: that it will increase the workload placed on scholars, that it will infringe on the traditional peer-review process, and that it will hurt the image of the discipline of psychology. Although implementing AR will add one more step to the bureaucratic publication process, we believe it can be implemented in an efficient manner that would greatly assist in decreasing the frequency and impact of misreporting while also providing secondary benefits in other domains of scientific integrity.},
  file = {/Users/tomhardwicke/pCloud Drive/Zotero_Library/Sakaluk_etal_2014.pdf},
  journal = {Perspectives on Psychological Science},
  language = {en},
  number = {6},
}
@Misc{jacoby2017,
  title = {Should {{Journals Be Responsible}} for {{Reproducibility}}? | {{Inside Higher Ed}}},
  shorttitle = {Should {{Journals Be Responsible}} for {{Reproducibility}}?},
  author = {W. G. Jacoby and S. Lafferty-Hess and T. M. Christian},
  year = {2017},
  abstract = {One of the top journals in political science makes data-sharing and replication part of the publication process.},
  file = {/Users/tomhardwicke/Zotero/storage/MVPCJLFC/should-journals-be-responsible-reproducibility.html},
  howpublished = {https://www.insidehighered.com/blogs/rethinking-research/should-journals-be-responsible-reproducibility},
  language = {en},
}
@Article{hardwicke2020b,
  title = {Calibrating the {{Scientific Ecosystem Through Meta}}-{{Research}}},
  author = {Tom E. Hardwicke and Stylianos Serghiou and Perrine Janiaud and Valentin Danchev and Sophia Cr{\"u}well and Steven N. Goodman and John P.A. Ioannidis},
  year = {2020},
  month = {mar},
  volume = {7},
  pages = {11--37},
  publisher = {{Annual Reviews}},
  issn = {2326-8298},
  doi = {10.1146/annurev-statistics-031219-041104},
  abstract = {While some scientists study insects, molecules, brains, or clouds, other scientists study science itself. Meta-research, or research-on-research, is a burgeoning discipline that investigates efficiency, quality, and bias in the scientific ecosystem, topics that have become especially relevant amid widespread concerns about the credibility of the scientific literature. Meta-research may help calibrate the scientific ecosystem toward higher standards by providing empirical evidence that informs the iterative generation and refinement of reform initiatives. We introduce a translational framework that involves (a) identifying problems, (b) investigating problems, (c) developing solutions, and (d) evaluating solutions. In each of these areas, we review key meta-research endeavors and discuss several examples of prior and ongoing work. The scientific ecosystem is perpetually evolving; the discipline of meta-research presents an opportunity to use empirical evidence to guide its development and maximize its potential.},
  file = {/Users/tomhardwicke/pCloud Drive/Zotero_Library/Hardwicke_etal_22.pdf;/Users/tomhardwicke/Zotero/storage/D46KIKI2/annurev-statistics-031219-041104.html},
  journal = {Annual Review of Statistics and Its Application},
  number = {1},
}
@Article{vuorre2020,
  title = {Sharing and Organizing Research Products as {{R}} Packages},
  author = {Matti Vuorre and Matthew Crump},
  year = {2020},
  month = {jan},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/jks2u},
  abstract = {A consensus on the importance of open data and reproducible code is emerging. How should data and code be shared to maximize the key desiderata of reproducibility, permanence, and accessibility? Research assets should be stored persistently in formats that are not software restrictive, and documented so that others can reproduce and extend the required computations. The sharing method should be easy to adopt by already busy researchers. We suggest the R package standard as a solution for creating, curating, and communicating research assets. The R package standard, with extensions discussed herein, provides a format for assets and metadata that satisfies the above desiderata, facilitates reproducibility, open access, and sharing of materials through online platforms like GitHub and Open Science Framework. We discuss a stack of R resources that help users create reproducible collections of research assets, from experiments to manuscripts, in the RStudio interface. We created an R package, vertical, to help researchers incorporate these tools into their workflows, and discuss its functionality at length in an online supplement. Together, these tools may increase the reproducibility and openness of psychological science.},
  file = {/Users/tomhardwicke/pCloud Drive/Zotero_Library/Vuorre_Crump_2020.pdf;/Users/tomhardwicke/Zotero/storage/6ZG89MWW/jks2u.html},
}
@Article{klein2018,
  title = {A {{Practical Guide}} for {{Transparency}} in {{Psychological Science}}},
  author = {Olivier Klein and Tom E. Hardwicke and Frederik Aust and Johannes Breuer and Henrik Danielsson and Alicia Hofelich Mohr and Hans Ijzerman and Gustav Nilsonne and Wolf Vanpaemel and Michael C. Frank},
  year = {2018},
  month = {jun},
  volume = {4},
  pages = {20},
  publisher = {{The Regents of the University of California}},
  issn = {2474-7394},
  doi = {10.1525/collabra.158},
  abstract = {Article: A Practical Guide for Transparency in Psychological Science},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are \textcopyright, \textregistered{} or \texttrademark{} of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  file = {/Users/tomhardwicke/pCloud Drive/Zotero_Library/Klein_etal_2018.pdf;/Users/tomhardwicke/Zotero/storage/9MUHFKY7/collabra.158.html},
  journal = {Collabra: Psychology},
  language = {eng},
  number = {1},
}
@Article{rouder2019,
  title = {Minimizing {{Mistakes}} in {{Psychological Science}}},
  author = {Jeffrey N. Rouder and Julia M. Haaf and Hope K. Snyder},
  year = {2019},
  month = {mar},
  volume = {2},
  pages = {3--11},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245918801915},
  abstract = {Developing and implementing best practices in organizing a lab is challenging, especially in the face of new cultural norms, such as the open-science movement. Part of this challenge in today's landscape is using new technologies, including cloud storage and computer automation. In this article, we discuss a few practices designed to increase the reliability of scientific labs, focusing on ways to minimize common, ordinary mistakes. We borrow principles from the theory of high-reliability organizations, which has been used to characterize operational practices in high-risk environments, such as aviation and health care. Guided by these principles, we focus on five strategies: (a) implementing a lab culture focused on learning from mistakes, (b) using computer automation in data and metadata collection whenever possible, (c) standardizing organizational strategies, (d) using coded rather than menu-driven analyses, and (e) developing expanded documents that record how analyses were performed.},
  journal = {Advances in Methods and Practices in Psychological Science},
  language = {en},
  number = {1},
}
@Article{naudet2018,
  title = {Data Sharing and Reanalysis of Randomized Controlled Trials in Leading Biomedical Journals with a Full Data Sharing Policy: Survey of Studies Published in {{{\emph{The BMJ}}}} and {{{\emph{PLOS Medicine}}}}},
  shorttitle = {Data Sharing and Reanalysis of Randomized Controlled Trials in Leading Biomedical Journals with a Full Data Sharing Policy},
  author = {Florian Naudet and Charlotte Sakarovitch and Perrine Janiaud and Ioana Cristea and Daniele Fanelli and David Moher and John P A Ioannidis},
  year = {2018},
  month = {feb},
  pages = {k400},
  issn = {0959-8138, 1756-1833},
  doi = {10.1136/bmj.k400},
  abstract = {OBJECTIVES To explore the effectiveness of data sharing by randomized controlled trials (RCTs) in journals with a full data sharing policy and to describe potential difficulties encountered in the process of performing reanalyses of the primary outcomes. DESIGN Survey of published RCTs. SETTING PubMed/Medline. ELIGIBILITY CRITERIA RCTs that had been submitted and published by The BMJ and PLOS Medicine subsequent to the adoption of data sharing policies by these journals.},
  file = {/Users/tomhardwicke/Zotero/storage/5KSA4BIL/Naudet et al. - 2018 - Data sharing and reanalysis of randomized controll.pdf},
  journal = {BMJ},
  language = {en},
}
@Article{borghi2020,
  title = {Data {{Management}} and {{Sharing}}: {{Practices}} and {{Perceptions}} of {{Psychology Researchers}}},
  shorttitle = {Data {{Management}} and {{Sharing}}},
  author = {John Borghi and Ana Van Gulick},
  year = {2020},
  month = {jun},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/7g3ae},
  abstract = {Research data is increasingly viewed as an important scholarly output. While a growing body of studies have investigated researcher practices and perceptions related to data sharing, information about data-related practices throughout the research process (including data collection and analysis) remains largely anecdotal. Building on our previous study of data practices in neuroimaging research, we conducted a survey of data management practices in the field of psychology. Our survey included questions about the type(s) of data collected, the tools used for data analysis, practices related to data organization, maintaining documentation, backup procedures, and long-term archiving of research materials. Our results demonstrate the complexity of managing and sharing data in psychology. Data is collected in multifarious forms from human participants, analyzed using a range of software tools, and archived in formats that may become obsolete. As individuals, our participants demonstrated relatively good data management practices, however they also indicated that there was little standardization within their research group. Participants generally indicated that they were willing to change their current practices in light of new technologies, opportunities, or requirements.},
  file = {/Users/tomhardwicke/pCloud Drive/Zotero_Library/Borghi_Gulick_2020.pdf;/Users/tomhardwicke/Zotero/storage/UWUJQQNC/7g3ae.html},
}
@Book{schwarzer2015,
  title = {Meta-{{Analysis}} with {{R}}},
  author = {Guido Schwarzer and James R. Carpenter and Gerta R{\"u}cker},
  year = {2015},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-319-21416-0},
  abstract = {This book provides a comprehensive introduction to performing meta-analysis using the statistical software R. It is intended for quantitative researchers and students in the medical and social sciences who wish to learn how to perform meta-analysis with R. As such, the book introduces the key concepts and models used in meta-analysis. It also includes chapters on the following advanced topics: publication bias and small study effects; missing data; multivariate meta-analysis, network meta-analysis; and meta-analysis of diagnostic studies.},
  file = {/Users/tomhardwicke/Zotero/storage/TRCKLEIX/9783319214153.html},
  isbn = {978-3-319-21415-3},
  language = {en},
  series = {Use {{R}}!},
}
@Article{minocher2020,
  title = {Reproducibility of Social Learning Research Declines Exponentially over 63 Years of Publication},
  author = {Riana Minocher and Silke Atmaca and Claudia Bavero and Richard McElreath and Bret Beheim},
  year = {2020},
  month = {jun},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/4nzc7},
  abstract = {Interest in improving reproducibility, replicability and transparency of research has increased substantially across scientific fields over the last few decades. We surveyed 560 empirical, quantitative publications published between 1955 and 2018, to estimate the rate of reproducibility for research on social learning, a large subfield of behavioural ecology. We found supporting materials were available for less than 30\% of publications during this period. The availability of data declines exponentially with time since publication, with a half-life of about six years, and this ``data decay rate'' varies systematically with both study design and study species. Conditional on materials being available, we estimate that a reasonable researcher could expect to successfully reproduce about 80\% of published results, based on our evaluating a subset of 40 publications. Taken together, this indicates an overall success rate of 24\% for both acquiring materials and recovering published results, with non- reproducibility of results primarily due to unavailable, incomplete, or poorly documented data. We provide recommendations to improve the reproducibility of research on the ecology and evolution of social behaviour.},
  file = {/Users/tomhardwicke/pCloud Drive/Zotero_Library/Minocher_etal_2020.pdf;/Users/tomhardwicke/Zotero/storage/CCYHRFA8/4nzc7.html},
}
@Book{field2012,
  title = {Discovering Statistics Using {{R}}},
  author = {Andy P. Field and Jeremy Miles and Zo{\"e} Field},
  year = {2012},
  publisher = {{Sage}},
  address = {{London ; Thousand Oaks, Calif}},
  isbn = {978-1-4462-0046-9 978-1-4462-0045-2},
  keywords = {Computer programs,R (Computer program language),Social sciences,Statistical methods Computer programs,Statistics},
  lccn = {HA32 .F537 2012},
}

@Article{newcombe1998,
  title = {Two-Sided Confidence Intervals for the Single Proportion: Comparison of Seven Methods},
  shorttitle = {Two-Sided Confidence Intervals for the Single Proportion},
  author = {Robert G. Newcombe},
  year = {1998},
  volume = {17},
  pages = {857--872},
  issn = {1097-0258},
  doi = {10.1002/(SICI)1097-0258(19980430)17:8<857::AID-SIM777>3.0.CO;2-E},
  abstract = {Simple interval estimate methods for proportions exhibit poor coverage and can produce evidently inappropriate intervals. Criteria appropriate to the evaluation of various proposed methods include: closeness of the achieved coverage probability to its nominal value; whether intervals are located too close to or too distant from the middle of the scale; expected interval width; avoidance of aberrations such as limits outside [0,1] or zero width intervals; and ease of use, whether by tables, software or formulae. Seven methods for the single proportion are evaluated on 96,000 parameter space points. Intervals based on tail areas and the simpler score methods are recommended for use. In each case, methods are available that aim to align either the minimum or the mean coverage with the nominal 1-{$\alpha$}. \textcopyright{} 1998 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 1998 John Wiley \& Sons, Ltd.},
  file = {/Users/tomhardwicke/Zotero/storage/3VH99G9L/(SICI)1097-0258(19980430)178857AID-SIM7773.0.html},
  journal = {Statistics in Medicine},
  language = {en},
  number = {8},
}

@Article{sison1995,
  title = {Simultaneous {{Confidence Intervals}} and {{Sample Size Determination}} for {{Multinomial Proportions}}},
  author = {Cristina P. Sison and Joseph Glaz},
  year = {1995},
  month = {mar},
  volume = {90},
  pages = {366--369},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1995.10476521},
  abstract = {Simultaneous confidence interval procedures for multinomial proportions are used in many areas of science. In this article two new simultaneous confidence interval procedures are introduced. Numerical results are presented to evaluate these procedures and compare their performance with established methods that have been used in statistical literature. From the results presented in this article, it is evident that the new procedures are more accurate than the established ones, where the accuracy of the procedure is measured by the volume of the confidence region corresponding to the nominal coverage probability and the probability of coverage it achieves. In the sample size determination problem, the new procedures provide a sizable amount of savings as compared to the procedures that have been used in many applications. Because both procedures performed equally well, the procedure that requires the least amount of computing time is recommended.},
  file = {/Users/tomhardwicke/Zotero/storage/PMG94VQ4/01621459.1995.html},
  journal = {Journal of the American Statistical Association},
  keywords = {Coverage probabilities,Multinomial distribution,Probability approximations,Simultaneous inference},
  number = {429},
}
@Misc{bastian2017,
  title = {Bias in {{Open Science Advocacy}}: {{The Case}} of {{Article Badges}} for {{Data Sharing}}},
  shorttitle = {Bias in {{Open Science Advocacy}}},
  author = {Hilda Bastian},
  year = {2017},
  month = {aug},
  abstract = {I like badges \textendash{} I have a lot of them! I'm also an open science advocate. So when a group of\ldots},
  file = {/Users/tomhardwicke/Zotero/storage/7X6ATVJZ/bias-in-open-science-advocacy-the-case-of-article-badges-for-data-sharing.html},
  journal = {Absolutely Maybe},
  language = {en-US},
}
